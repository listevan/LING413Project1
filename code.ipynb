{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7ac6940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num_Classes: 15, Classes: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liste\\miniconda3\\envs\\CS445Final\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import brown\n",
    "NUM_CLASSES = len(brown.categories())\n",
    "print(f\"Num_Classes: {NUM_CLASSES}, Classes: {brown.categories()}\")\n",
    "\n",
    "from datasets import Dataset\n",
    "data_dict = {\n",
    "    \"text\": [\n",
    "        \" \".join(sent)\n",
    "        for file_id in brown.fileids()\n",
    "        # Corrected line: 'fileid' changed to 'fileids'\n",
    "        for sent in brown.sents(fileids=file_id)\n",
    "    ],\n",
    "    \"label\": [\n",
    "        brown.categories(fileids=file_id)[0]\n",
    "        for file_id in brown.fileids()\n",
    "        # Corrected line: 'fileid' changed to 'fileids'\n",
    "        for sent in brown.sents(fileids=file_id)\n",
    "    ]\n",
    "}\n",
    "\n",
    "full_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "from datasets import DatasetDict\n",
    "train_val_split = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_val_ds = train_val_split['train'] # This is 90% of the data\n",
    "test_ds = train_val_split['test']     # This is 10% of the data\n",
    "\n",
    "val_split = train_val_ds.train_test_split(test_size=(1/9), seed=42)\n",
    "\n",
    "train_ds = val_split['train']\n",
    "val_ds = val_split['test']\n",
    "\n",
    "final_splits = DatasetDict({\n",
    "    'train': train_ds,\n",
    "    'validation': val_ds,\n",
    "    'test': test_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5aeaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d192c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45872/45872 [00:07<00:00, 6383.70 examples/s]\n",
      "Map: 100%|██████████| 5734/5734 [00:00<00:00, 6336.86 examples/s]\n",
      "Map: 100%|██████████| 5734/5734 [00:00<00:00, 6756.65 examples/s]\n",
      "Casting to class labels: 100%|██████████| 45872/45872 [00:00<00:00, 300525.78 examples/s]\n",
      "Casting to class labels: 100%|██████████| 5734/5734 [00:00<00:00, 381717.95 examples/s]\n",
      "Casting to class labels: 100%|██████████| 5734/5734 [00:00<00:00, 337006.60 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = final_splits.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.class_encode_column(\"label\")\n",
    "\n",
    "label_names = tokenized_dataset[\"train\"].features[\"label\"].names\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b98c1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'But exactly how far it will go toward improving conditions is another question because there is so much that needs doing .', 'label': 2, 'input_ids': [101, 2021, 3599, 2129, 2521, 2009, 2097, 2175, 2646, 9229, 3785, 2003, 2178, 3160, 2138, 2045, 2003, 2061, 2172, 2008, 3791, 2725, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93fa6039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=NUM_CLASSES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44363210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class RegisterClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(RegisterClassifier, self).__init__()\n",
    "        # Load the pretrained BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        \n",
    "        # Fully-connected layer for classification\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass inputs through BERT\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use the pooled output of the token for classification\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Apply dropout and the final classification layer\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2cd21b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liste\\AppData\\Local\\Temp\\ipykernel_15936\\478141127.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08cbb71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8601' max='8601' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8601/8601 2:15:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.470400</td>\n",
       "      <td>1.345485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.009300</td>\n",
       "      <td>1.223019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.730600</td>\n",
       "      <td>1.264068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "import torch\n",
    "# Assuming 'model' is your trained PyTorch model\n",
    "# and 'output_dir' is the directory to save the model\n",
    "output_dir = \"model_weights\"\n",
    "model_save_path = f\"{output_dir}/final_model_weights.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2a9cb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\liste\\AppData\\Local\\Temp\\ipykernel_15936\\808949477.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Instantiate the same model architecture\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Load the saved weights\n",
    "output_dir = \"model_weights\"\n",
    "model_save_path = f\"{output_dir}/final_model_weights.pth\"\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a655d9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      adventure       0.49      0.58      0.53       471\n",
      " belles_lettres       0.57      0.63      0.60       722\n",
      "      editorial       0.41      0.38      0.39       298\n",
      "        fiction       0.54      0.46      0.49       476\n",
      "     government       0.72      0.75      0.73       303\n",
      "        hobbies       0.76      0.71      0.73       387\n",
      "          humor       0.34      0.23      0.28        94\n",
      "        learned       0.79      0.76      0.78       727\n",
      "           lore       0.55      0.59      0.57       504\n",
      "        mystery       0.57      0.54      0.55       408\n",
      "           news       0.67      0.66      0.67       449\n",
      "       religion       0.68      0.59      0.63       182\n",
      "        reviews       0.62      0.52      0.57       181\n",
      "        romance       0.44      0.52      0.48       440\n",
      "science_fiction       0.69      0.41      0.52        92\n",
      "\n",
      "       accuracy                           0.60      5734\n",
      "      macro avg       0.59      0.56      0.57      5734\n",
      "   weighted avg       0.60      0.60      0.60      5734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test_results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "# print(\"\\nTest Set Evaluation Results:\")\n",
    "# for key, value in test_results.items():\n",
    "#     print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "import numpy as np\n",
    "predictions_output = trainer.predict(tokenized_dataset[\"test\"])\n",
    "predicted_labels = np.argmax(predictions_output.predictions, axis=1)\n",
    "true_labels = tokenized_dataset[\"test\"][\"label\"]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=label_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f7098e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45872/45872 [00:02<00:00, 17884.16 examples/s]\n",
      "Map: 100%|██████████| 5734/5734 [00:00<00:00, 19397.07 examples/s]\n",
      "Map: 100%|██████████| 5734/5734 [00:00<00:00, 14442.62 examples/s]\n",
      "Casting to class labels: 100%|██████████| 45872/45872 [00:00<00:00, 475783.44 examples/s]\n",
      "Casting to class labels: 100%|██████████| 5734/5734 [00:00<00:00, 457279.14 examples/s]\n",
      "Casting to class labels: 100%|██████████| 5734/5734 [00:00<00:00, 476816.34 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "Loading pre-trained BERT model for embeddings...\n",
      "\n",
      "----- Training Simple Neural Network (NN) with BERT Embeddings -----\n",
      "Epoch 01 | Train Loss: 1.828 | Val. Loss: 1.630 | Val. Acc: 43.91%\n",
      "Epoch 02 | Train Loss: 1.692 | Val. Loss: 1.594 | Val. Acc: 45.17%\n",
      "Epoch 03 | Train Loss: 1.644 | Val. Loss: 1.550 | Val. Acc: 47.04%\n",
      "\n",
      "----- Testing Simple Neural Network (NN) with BERT Embeddings -----\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      adventure       0.40      0.32      0.36       471\n",
      " belles_lettres       0.40      0.65      0.50       722\n",
      "      editorial       0.36      0.29      0.32       298\n",
      "        fiction       0.35      0.15      0.21       476\n",
      "     government       0.59      0.55      0.57       303\n",
      "        hobbies       0.58      0.59      0.58       387\n",
      "          humor       0.25      0.01      0.02        94\n",
      "        learned       0.68      0.65      0.66       727\n",
      "           lore       0.50      0.26      0.35       504\n",
      "        mystery       0.30      0.52      0.38       408\n",
      "           news       0.53      0.61      0.57       449\n",
      "       religion       0.56      0.34      0.42       182\n",
      "        reviews       0.66      0.28      0.40       181\n",
      "        romance       0.29      0.47      0.36       440\n",
      "science_fiction       0.56      0.16      0.25        92\n",
      "\n",
      "       accuracy                           0.45      5734\n",
      "      macro avg       0.47      0.39      0.40      5734\n",
      "   weighted avg       0.47      0.45      0.44      5734\n",
      "\n",
      "----- End of Simple Neural Network (NN) with BERT Embeddings Experiment -----\n",
      "\n",
      "----- Training Text CNN with BERT Embeddings -----\n",
      "Epoch 01 | Train Loss: 1.811 | Val. Loss: 1.574 | Val. Acc: 45.52%\n",
      "Epoch 02 | Train Loss: 1.654 | Val. Loss: 1.510 | Val. Acc: 47.92%\n",
      "Epoch 03 | Train Loss: 1.582 | Val. Loss: 1.472 | Val. Acc: 49.69%\n",
      "\n",
      "----- Testing Text CNN with BERT Embeddings -----\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      adventure       0.36      0.58      0.44       471\n",
      " belles_lettres       0.44      0.64      0.52       722\n",
      "      editorial       0.38      0.38      0.38       298\n",
      "        fiction       0.37      0.15      0.21       476\n",
      "     government       0.59      0.65      0.62       303\n",
      "        hobbies       0.72      0.53      0.61       387\n",
      "          humor       0.57      0.13      0.21        94\n",
      "        learned       0.78      0.57      0.66       727\n",
      "           lore       0.45      0.40      0.42       504\n",
      "        mystery       0.47      0.25      0.32       408\n",
      "           news       0.54      0.63      0.58       449\n",
      "       religion       0.57      0.42      0.48       182\n",
      "        reviews       0.58      0.41      0.48       181\n",
      "        romance       0.31      0.54      0.39       440\n",
      "science_fiction       0.76      0.27      0.40        92\n",
      "\n",
      "       accuracy                           0.48      5734\n",
      "      macro avg       0.52      0.44      0.45      5734\n",
      "   weighted avg       0.51      0.48      0.47      5734\n",
      "\n",
      "----- End of Text CNN with BERT Embeddings Experiment -----\n",
      "\n",
      "----- Training Bidirectional LSTM with BERT Embeddings -----\n",
      "Epoch 01 | Train Loss: 1.815 | Val. Loss: 1.668 | Val. Acc: 42.24%\n",
      "Epoch 02 | Train Loss: 1.659 | Val. Loss: 1.592 | Val. Acc: 44.96%\n",
      "Epoch 03 | Train Loss: 1.555 | Val. Loss: 1.513 | Val. Acc: 47.66%\n",
      "\n",
      "----- Testing Bidirectional LSTM with BERT Embeddings -----\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      adventure       0.40      0.34      0.37       471\n",
      " belles_lettres       0.46      0.57      0.51       722\n",
      "      editorial       0.46      0.24      0.32       298\n",
      "        fiction       0.29      0.46      0.35       476\n",
      "     government       0.57      0.69      0.62       303\n",
      "        hobbies       0.67      0.51      0.58       387\n",
      "          humor       0.25      0.01      0.02        94\n",
      "        learned       0.71      0.66      0.68       727\n",
      "           lore       0.45      0.43      0.44       504\n",
      "        mystery       0.34      0.55      0.42       408\n",
      "           news       0.60      0.55      0.58       449\n",
      "       religion       0.49      0.43      0.46       182\n",
      "        reviews       0.58      0.37      0.45       181\n",
      "        romance       0.35      0.29      0.32       440\n",
      "science_fiction       0.82      0.20      0.32        92\n",
      "\n",
      "       accuracy                           0.47      5734\n",
      "      macro avg       0.50      0.42      0.43      5734\n",
      "   weighted avg       0.49      0.47      0.47      5734\n",
      "\n",
      "----- End of Bidirectional LSTM with BERT Embeddings Experiment -----\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from datasets import Dataset, DatasetDict\n",
    "from nltk.corpus import brown\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- 2. Tokenization ---\n",
    "print(\"\\nTokenizing the dataset...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "\n",
    "tokenized_dataset = final_splits.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.class_encode_column(\"label\")\n",
    "label_names = tokenized_dataset[\"train\"].features[\"label\"].names\n",
    "\n",
    "# Prepare dataset for PyTorch\n",
    "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "\n",
    "# --- 3. Hyperparameters and Setup ---\n",
    "HIDDEN_DIM = 256\n",
    "NUM_EPOCHS = 3 # Fewer epochs needed as BERT features are very strong\n",
    "BATCH_SIZE = 32 # Smaller batch size for BERT\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT_PROB = 0.5\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Load Pre-trained BERT model for embedding generation\n",
    "print(\"Loading pre-trained BERT model for embeddings...\")\n",
    "bert_embedder = BertModel.from_pretrained('bert-base-uncased')\n",
    "# Freeze BERT parameters to use it only as a feature extractor\n",
    "for param in bert_embedder.parameters():\n",
    "    param.requires_grad = False\n",
    "bert_embedder.to(device)\n",
    "\n",
    "BERT_HIDDEN_SIZE = bert_embedder.config.hidden_size\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(tokenized_dataset[\"validation\"], batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=BATCH_SIZE)\n",
    "\n",
    "# --- 4. Model Definitions with BERT Embeddings ---\n",
    "\n",
    "# Simple Neural Network (NN) with BERT Embeddings\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, bert_embedder, hidden_dim, output_dim, dropout_prob):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.bert_embedder = bert_embedder\n",
    "        self.fc1 = nn.Linear(BERT_HIDDEN_SIZE, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            bert_output = self.bert_embedder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embedded = bert_output.last_hidden_state\n",
    "        pooled = embedded.mean(dim=1)\n",
    "        hidden = self.relu(self.fc1(pooled))\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc2(hidden)\n",
    "\n",
    "# Convolutional Neural Network (CNN) with BERT Embeddings\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, bert_embedder, n_filters, filter_sizes, output_dim, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.bert_embedder = bert_embedder\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=BERT_HIDDEN_SIZE, out_channels=n_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            bert_output = self.bert_embedder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embedded = bert_output.last_hidden_state.permute(0, 2, 1)\n",
    "        \n",
    "        conved = [torch.relu(conv(embedded)) for conv in self.convs]\n",
    "        pooled = [torch.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        return self.fc(cat)\n",
    "\n",
    "# Bidirectional LSTM with BERT Embeddings\n",
    "class TextLSTM(nn.Module):\n",
    "    def __init__(self, bert_embedder, hidden_dim, output_dim, n_layers, bidirectional, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.bert_embedder = bert_embedder\n",
    "        self.lstm = nn.LSTM(\n",
    "            BERT_HIDDEN_SIZE,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob if n_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            bert_output = self.bert_embedder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embedded = bert_output.last_hidden_state\n",
    "\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "            \n",
    "        return self.fc(hidden)\n",
    "\n",
    "# --- 5. Training and Evaluation Functions ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(input_ids, attention_mask)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted_labels = torch.max(predictions, dim=1)\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test_model(model, dataloader, device, label_names):\n",
    "    model.eval()\n",
    "    predictions_list, true_labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            _, predicted_labels = torch.max(predictions, dim=1)\n",
    "            \n",
    "            predictions_list.extend(predicted_labels.cpu().numpy())\n",
    "            true_labels_list.extend(labels.cpu().numpy())\n",
    "            \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels_list, predictions_list, target_names=label_names, zero_division=0))\n",
    "\n",
    "# --- 6. Run Training and Testing ---\n",
    "def run_experiment(model, model_name):\n",
    "    print(f\"\\n----- Training {model_name} -----\")\n",
    "    model.to(device)\n",
    "    # Only train the parameters of the classification head, not the BERT embedder\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n",
    "        print(f'Epoch {epoch+1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {val_loss:.3f} | Val. Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "    print(f\"\\n----- Testing {model_name} -----\")\n",
    "    test_model(model, test_dataloader, device, label_names)\n",
    "    print(f\"----- End of {model_name} Experiment -----\")\n",
    "\n",
    "# Run for Simple NN\n",
    "nn_model = SimpleNN(bert_embedder, HIDDEN_DIM, NUM_CLASSES, DROPOUT_PROB)\n",
    "run_experiment(nn_model, \"Simple Neural Network (NN) with BERT Embeddings\")\n",
    "\n",
    "# Run for Text CNN\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2, 3, 4]\n",
    "cnn_model = TextCNN(bert_embedder, N_FILTERS, FILTER_SIZES, NUM_CLASSES, DROPOUT_PROB)\n",
    "run_experiment(cnn_model, \"Text CNN with BERT Embeddings\")\n",
    "\n",
    "# Run for Bidirectional LSTM\n",
    "LSTM_LAYERS = 2\n",
    "lstm_model = TextLSTM(bert_embedder, HIDDEN_DIM, NUM_CLASSES, LSTM_LAYERS, True, DROPOUT_PROB)\n",
    "run_experiment(lstm_model, \"Bidirectional LSTM with BERT Embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81d02059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Saving Final Model Weights -----\n",
      "Simple NN weights saved to model_weights\\simple_nn_weights.pth\n",
      "Text CNN weights saved to model_weights\\text_cnn_weights.pth\n",
      "Bidirectional LSTM weights saved to model_weights\\bidirectional_lstm_weights.pth\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Save Model Weights ---\n",
    "print(\"\\n----- Saving Final Model Weights -----\")\n",
    "output_dir = \"model_weights\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define paths for the weights\n",
    "nn_save_path = os.path.join(output_dir, \"simple_nn_weights.pth\")\n",
    "cnn_save_path = os.path.join(output_dir, \"text_cnn_weights.pth\")\n",
    "lstm_save_path = os.path.join(output_dir, \"bidirectional_lstm_weights.pth\")\n",
    "\n",
    "# Save the state dictionaries\n",
    "torch.save(nn_model.state_dict(), nn_save_path)\n",
    "print(f\"Simple NN weights saved to {nn_save_path}\")\n",
    "\n",
    "torch.save(cnn_model.state_dict(), cnn_save_path)\n",
    "print(f\"Text CNN weights saved to {cnn_save_path}\")\n",
    "\n",
    "torch.save(lstm_model.state_dict(), lstm_save_path)\n",
    "print(f\"Bidirectional LSTM weights saved to {lstm_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d9201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LING413",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
